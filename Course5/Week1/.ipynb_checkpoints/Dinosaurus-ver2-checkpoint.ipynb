{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the original exercise uses the numpy built methods from the previous exercise to construct a dino-name generator, I will try to use Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, SimpleRNN, Dense, Lambda, LSTM, Masking, TimeDistributed\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many charactars in the our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file again, and read the lines\n",
    "with open(\"dinos.txt\") as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]\n",
    "# Shuffle list of all dinosaur names\n",
    "np.random.seed(210)\n",
    "np.random.shuffle(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# compute the max length of a word in data\n",
    "maxLen = len(max(examples, key=len))\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(examples)\n",
    "feature_size = 1  # we use the actual index, and not 1-hot-encoding\n",
    "n_a = 50  # latent space dimensions\n",
    "\n",
    "X = np.zeros((m, maxLen, feature_size))\n",
    "Y = np.zeros((m, maxLen, feature_size))\n",
    "\n",
    "for i in range(m):\n",
    "    word = examples[i]\n",
    "    l = len(word)        \n",
    "    X[i, 0:l, :] = np.array([char_to_ix[ch] for ch in word]).reshape(-1, 1)\n",
    "    Y[i, 0:l, :] = np.concatenate([X[i, 1:l, :], [[char_to_ix[\"\\n\"]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "def simple_model(input_shape, n_a):\n",
    "    inp = Input(shape=input_shape, name='x')\n",
    "    X = Masking(mask_value=0)(inp)\n",
    "    X = SimpleRNN(n_a, return_sequences=True, name='rnn_cell')(X)\n",
    "    X = Dense(vocab_size, activation='softmax', name='dense_output')(X)\n",
    "    model = Model(inputs=inp, outputs=X)\n",
    "    return model\n",
    "\n",
    "try:\n",
    "    model = load_model('simple_rnn.h5')\n",
    "except:\n",
    "    model = simple_model(input_shape=(maxLen, feature_size))\n",
    "    # Use sparse loss due to not using 1-hot-encoding\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "    # learning_rate_reduction = ReduceLROnPlateau(monitor='sparse_categorical_accuracy', patience=4, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "    model.fit(X, Y, epochs = 200, batch_size = 128) # , callbacks=[learning_rate_reduction]\n",
    "    model.save('simple_rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 45% accuracy after 100-200 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build an \"inference\" generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_model(model):\n",
    "    inp = Input(shape=(1, feature_size)) # regular input - char\n",
    "    state_h = Input(shape=(n_a,))  # state input, a n_a dimensions vector\n",
    "    \n",
    "    inf_rnn = SimpleRNN(n_a, return_sequences=True, return_state=True)   \n",
    "    X, h = inf_rnn(inp, initial_state=[state_h])\n",
    "    dense = model.get_layer(name='dense_output')  # use the already trained dense layer\n",
    "    X = dense(X)\n",
    "    model_infer = Model(inputs=[inp, state_h], outputs=[X, h])    \n",
    "    rnn = model.get_layer(name='rnn_cell')\n",
    "    inf_rnn.set_weights(rnn.get_weights()) # load the weights of the trained rnn\n",
    "    return model_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(inf_model, seq_len=20):\n",
    "    x = np.random.randint(1, vocab_size, size=(1, 1, 1))\n",
    "    name = []\n",
    "    state_h = np.zeros((1,n_a))\n",
    "    for _ in range(seq_len):\n",
    "        name.append(ix_to_char[x[0][0][0]])\n",
    "        prob, state_h = inf_model.predict([x, state_h])  # get the softmax output, handle as a distribution\n",
    "        pred = np.random.choice(np.arange(vocab_size), p=prob.ravel())  # sample from the distribution \n",
    "        x = pred.reshape(1,1,1)\n",
    "        if x[0][0][0] == 0:  # if we reach a new line, we break\n",
    "            break\n",
    "        \n",
    "    return \"\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model = get_inference_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rueinpigus'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name(inf_model, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Let's try LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "n_a = 128\n",
    "def lstm_model(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    X = Masking(mask_value=0)(inp)\n",
    "    X = LSTM(n_a, return_sequences=True, name='lstm_cell')(X)\n",
    "    X = Dense(vocab_size, activation='softmax', name='dense_output_lstm')(X)\n",
    "    model = Model(inputs=inp, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    lstm = load_model('lstm.h5')\n",
    "except:\n",
    "    lstm = lstm_model(input_shape=(maxLen, feature_size))\n",
    "    lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "    lstm.fit(X, Y, epochs = 200, batch_size = 128)\n",
    "    lstm.save('lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_model_lstm(model):\n",
    "    inp = Input(shape=(1, feature_size)) # regular input - char\n",
    "    state_h = Input(shape=(n_a,))  # state input, a n_a dimensions vector\n",
    "    state_c = Input(shape=(n_a,))  # memory input, a n_a dimensions vector\n",
    "\n",
    "    inf_lstm = LSTM(n_a, return_sequences=True, return_state=True)   \n",
    "    X, h, c = inf_lstm(inp, initial_state=[state_h, state_c])\n",
    "    dense = model.get_layer(name='dense_output_lstm')  # use the already trained dense layer\n",
    "    X = dense(X)\n",
    "    model_infer = Model(inputs=[inp, state_h, state_c], outputs=[X, h, c])    \n",
    "    lstm = model.get_layer(name='lstm_cell')\n",
    "    inf_lstm.set_weights(lstm.get_weights()) # load the weights of the trained rnn\n",
    "    return model_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_lstm(inf_model, seq_len=20):\n",
    "    x = np.random.randint(1, vocab_size, size=(1, 1, 1))\n",
    "    name = []\n",
    "    state_h = np.zeros((1,n_a))\n",
    "    state_c = np.zeros((1,n_a))\n",
    "    for _ in range(seq_len):\n",
    "        name.append(ix_to_char[x[0][0][0]])\n",
    "        prob, state_h, state_c = inf_model.predict([x, state_h, state_c])  # get the softmax output, handle as a distribution\n",
    "        pred = np.random.choice(np.arange(vocab_size), p=prob.ravel())  # sample from the distribution \n",
    "        x = pred.reshape(1,1,1)\n",
    "        if x[0][0][0] == 0:  # if we reach a new line, we break\n",
    "            break\n",
    "        \n",
    "    return \"\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model = get_inference_model_lstm(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xleosaurus'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name_lstm(inf_model, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use an even more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_model(input_shape, n_a = 128):\n",
    "    inp = Input(shape=input_shape, name='x')\n",
    "    X = Masking(mask_value=0)(inp)\n",
    "    X = LSTM(n_a, return_sequences=True, name='lstm_1')(X)\n",
    "    X = LSTM(n_a//2, return_sequences=True, name='lstm_2')(X)\n",
    "    X = LSTM(n_a//3, return_sequences=True, name='lstm_3')(X)\n",
    "    X = Dense(vocab_size, activation='softmax', name='dense_output_lstm')(X)\n",
    "    model = Model(inputs=inp, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    complx = load_model('complex.h5')\n",
    "except:\n",
    "    complx = complex_model(input_shape=(maxLen, feature_size))\n",
    "    complx.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "    complx.fit(X, Y, epochs = 200, batch_size = 256)\n",
    "    complx.save('complex.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_model_complex(model):\n",
    "    inp = Input(shape=(1, feature_size)) # regular input - char\n",
    "    state_h1 = Input(shape=(n_a,))  # state input, a n_a dimensions vector\n",
    "    state_c1 = Input(shape=(n_a,))  # memory input, a n_a dimensions vector\n",
    "    state_h2 = Input(shape=(n_a//2,))  # state input, a n_a/2 dimensions vector\n",
    "    state_c2 = Input(shape=(n_a//2,))  # memory input, a n_a/2 dimensions vector\n",
    "    state_h3 = Input(shape=(n_a//3,))  # state input, a n_a/3 dimensions vector\n",
    "    state_c3 = Input(shape=(n_a//3,))  # memory input, a n_a/3 dimensions vector\n",
    "\n",
    "    inf_lstm1 = LSTM(n_a, return_sequences=True, return_state=True)   \n",
    "    X, h1, c1 = inf_lstm1(inp, initial_state=[state_h1, state_c1])\n",
    "    inf_lstm2 = LSTM(n_a//2, return_sequences=True, return_state=True)   \n",
    "    X, h2, c2 = inf_lstm2(X, initial_state=[state_h2, state_c2])\n",
    "    inf_lstm3 = LSTM(n_a//3, return_sequences=True, return_state=True)   \n",
    "    X, h3, c3 = inf_lstm3(X, initial_state=[state_h3, state_c3])\n",
    "    \n",
    "    dense = model.get_layer(name='dense_output_lstm')  # use the already trained dense layer\n",
    "    X = dense(X)\n",
    "    model_infer = Model(inputs=[inp, state_h1, state_c1, state_h2, state_c2, state_h3, state_c3], \n",
    "                        outputs=[X, h1, c1, h2, c2, h3, c3])    \n",
    "    \n",
    "    lstm1 = model.get_layer(name='lstm_1')\n",
    "    inf_lstm1.set_weights(lstm1.get_weights()) # load the weights of the trained rnn\n",
    "    lstm2 = model.get_layer(name='lstm_2')\n",
    "    inf_lstm2.set_weights(lstm2.get_weights()) # load the weights of the trained rnn\n",
    "    lstm3 = model.get_layer(name='lstm_3')\n",
    "    inf_lstm3.set_weights(lstm3.get_weights()) # load the weights of the trained rnn\n",
    "    \n",
    "    return model_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_complex(inf_model, seq_len=20):\n",
    "    x = np.random.randint(1, vocab_size, size=(1, 1, 1))\n",
    "    name = []\n",
    "    state_h1 = np.zeros((1,n_a))\n",
    "    state_c1 = np.zeros((1,n_a))\n",
    "    state_h2 = np.zeros((1,n_a//2))\n",
    "    state_c2 = np.zeros((1,n_a//2))\n",
    "    state_h3 = np.zeros((1,n_a//3))\n",
    "    state_c3 = np.zeros((1,n_a//3))\n",
    "\n",
    "    for _ in range(seq_len):\n",
    "        name.append(ix_to_char[x[0][0][0]])\n",
    "        prob, state_h1, state_c1, state_h2, state_c2, state_h3, state_c3 = inf_model.predict(\n",
    "            [x, state_h1, state_c1, state_h2, state_c2, state_h3, state_c3])  # get the softmax output, handle as a distribution\n",
    "        pred = np.random.choice(np.arange(vocab_size), p=prob.ravel())  # sample from the distribution \n",
    "        x = pred.reshape(1,1,1)\n",
    "        if x[0][0][0] == 0:  # if we reach a new line, we break\n",
    "            break\n",
    "        \n",
    "    return \"\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmplx_inf_model = get_inference_model_complex(complx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zetoaensaurus'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name_complex(cmplx_inf_model, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    Takes softmax (multinomial) probabilities and converts them either to less diversity or more, \n",
    "    then samples from this distribution and returns the sampled number\n",
    "    '''\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
